El campo de \textbf{Análisis y Diseño de algoritmos en Ciencias de la Computación} es un campo que busca analizar y diseñar algoritmos para resolver varios problemas reales cotidianos e industriales como el ordenamiento, la matemática, el recorrido de mapas y laberintos, multiplicación de matrices, etc. y hacerlos de las formas más eficientes posibles.

Se han creado y analizado varios algoritmos para estos problemas cuya complejidad usualmente evoluciona según aumenta el tamaño de la entrada. Pero, \textbf{¿cómo se manifiestan la complejidades de tales algoritmos en la práctica al verse implementados en código? ¿Cómo se comparan para distintos tamaños de entrada al ejecutarse?}

Recordar que para el análisis de complejidad de algoritmos existen las notaciones $O(f(n))$ y $\Omega(f(n))$, las cuales describen las cotas superiores e inferiores de complejidades temporal y/o espacial de algún algoritmo $f$, siendo éste un algoritmo cualquiera que resuelve un problema particular $A$. Sin embargo, esto \textit{no nos dice nada} sobre el tiempo de ejecución o utilización de memoria brutos de un algoritmo, sólo el cómo evoluciona. Entonces, nuestra hipótesis es que es perfectamente posible que un algoritmo $f$ con peor complejidad temporal o espacial que otro algoritmo $g$ (siendo $g$ un algoritmo que resuelve el mismo problema que $f$) pueda ser más rápido que éste último hasta algún valor $k \in \mathbb{N}$.

Por lo anterior, el propósito de este informe es probar la hipótesis recién dada, y además ayudarnos a entender mejor los tiempos de ejecución y utilizaciones de memoria de varios algoritmos, y ver la practicalidad de éstos para distintos tamaños de $n$. Este informe se enfocará en cuatro algoritmos de ordenamiento por comparación (std::sort, Selection Sort, Merge Sort, y Quick Sort) y dos algoritmos de multiplicación de matrices cuadradas (Naive y Strassen).